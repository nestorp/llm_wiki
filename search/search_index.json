{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LLM Wiki","text":"<p>Welcome to the LLM-Wiki!</p> <p>This repository is dedicated to curating and compiling information on large language models (LLMs). Dive deep into the cutting-edge research, methodologies, and breakthroughs in the domain of LLMs. Additionally, explore real-world applications, case studies, and best practices for implementing these models. Whether you're a researcher, developer, or enthusiast, this wiki is intended to serve as a one-stop resource for all things related to large language models.</p>"},{"location":"Companies/Anthropic/","title":"Anthropic","text":"<p>Anthropic PBC, is an AI startup established with a primary focus on the development of general AI systems and language models. The company was founded in 2021 by former senior members of OpenAI, including siblings Daniela Amodei and Dario Amodei. The latter had previously held the position of Vice President of Research at OpenAI. The establishment of Anthropic was influenced by directional differences with OpenAI, particularly concerning OpenAI's collaborations with Microsoft in 2019.</p> <p>A significant product of Anthropic's research and development efforts is the AI chatbot named Claude. Drawing from the expertise of researchers who had previously been involved in the development of OpenAI's [[GPT-2]] and GPT-3 models, Claude was designed to function through a messaging interface. Users can pose questions or requests to Claude and receive detailed and pertinent responses. </p> <p>Claude was initially made available to a select group through a Slack integration during its beta phase. However, it has since been made accessible to a broader audience via its dedicated website, claude.ai</p> <p>In July 2023, Anthropic launched Claude 2, expanding its availability to both the US and UK markets. During the development of this iteration, safety was emphasised, leading to Anthropic's characterisation of the model as \"Constitutional AI\".</p>"},{"location":"Companies/Google%20AI/","title":"Google AI","text":"<p>Google AI, established in 2017, is a specialised division within Google, dedicated to advancements in AI. A significant organisational change occurred in 2023 when Google AI underwent a reorganisation. This initiative elevated Jeff Dean, the head of Google AI, to the role of chief scientist at Google. A notable aspect of this reorganisation was the merger of Google Brain and DeepMind, a UK-based AI company acquired by Google in 2014. Prior to this merger, DeepMind operated autonomously from Google's core research activities.</p> <p>Google AI has developed a conversational generative artificial intelligence chatbot named Bard.</p>"},{"location":"Companies/Hugging%20Face/","title":"Hugging Face","text":"<p>Hugging Face, Inc. was founded in 2016 in New York City by French entrepreneurs Cl\u00e9ment Delangue, Julien Chaumond, and Thomas Wolf. Delangue serves as the CEO, Chaumond as the CTO, and Wolf as the CSO. The company is known for its development of tools and platforms for machine learning applications. </p> <p>Hugging Face's most notable contribution is the Transformers library, a Python package tailored for NLP tasks. This library offers open-source implementations of transformer models for text, image, and audio tasks. It is compatible with major deep learning frameworks, including [[PyTorch]], [[TensorFlow]], and [[JAX]]. Other important contributions are the Datasets library and the Huggingface Hub a centralized platform facilitates the hosting and sharing of machine learning models and datasets.</p>"},{"location":"Companies/Meta%20AI/","title":"Meta AI","text":"<p>Meta AI, an artificial intelligence laboratory under Meta Platforms Inc. (formerly Facebook, Inc.), establised in 2015. It was previously known as Facebook Artificial Intelligence Research (FAIR).</p> <p>In 2017, FAIR released [[PyTorch]], an open-source machine learning framework.</p> <p>In 2023, Meta AI announced and open-sourced LLaMA (Large Language Model Meta AI), a 65B parameter large language model.</p>"},{"location":"Companies/MosaicML/","title":"MosaicML","text":"<p>MosaicML is an open-source generative AI startup that was acquired by Databricks in May 2023.</p> <p>They developed MPT.</p>"},{"location":"Companies/OpenAI/","title":"OpenAI","text":"<p>OpenAI is an AI research laboratory that encompasses both a non-profit entity, OpenAI, Inc., and its for-profit subsidiary, OpenAI, L.P. Established on December 10, 2015, OpenAI's mission revolves around the creation of \"safe and beneficial\" artificial general intelligence (AGI), which they define as \"highly autonomous systems that outperform humans at most economically valuable work.\"</p> <p>OpenAI has developed several AI systems and models, including:</p> <ul> <li>ChatGPT: Launched on November 30, 2022, ChatGPT is a large language model-based chatbot. It enables users to steer conversations in terms of length, format, style, detail, and language. Built on GPT-3.5 and GPT-4 ChatGPT has been fine-tuned for conversational applications. By January 2023, it had amassed over 100 million users.</li> <li>[[OpenAI Codex]]: This AI model interprets natural language and produces code. It powers GitHub Copilot, a programming autocompletion tool. Codex is a descendant of OpenAI's GPT-3 model, tailored for programming applications.</li> </ul>"},{"location":"Guides/LLM%20Fine-tuning/","title":"LLM Fine tuning","text":"<p>This guide sets out to create general guidelines for locally fine-tuning pre-trained large language models. It outlines the available tools, datasets and models that can be used and makes some recommendations. It also points to additional resources that can be useful for further understanding the fine-tuning process.</p>"},{"location":"Guides/LLM%20Fine-tuning/#hardware","title":"Hardware","text":"<p>The rule of thumb is you need (Model Parameters in Billions) x 4 in GPU RAM GB for inference. Eg. for the Llama 2 7B version, you would need 7x4 = 28 GB of GPU RAM. For fine-tuning, the requirements increase up to double this amount, depending on the optimiser used. </p> <p>Low Rank Fine-tuning (LoRa) and Quantization are techniques aimed at optimising the use of resources by language models during both inference and training. They can allow for more efficient fine-tuning requiring fewer resources.</p>"},{"location":"Guides/LLM%20Fine-tuning/#low-rank-fine-tuning-lora","title":"Low Rank Fine-tuning (LoRa)","text":"<p>Low Rank Fine-tuning (LoRa) is a method designed to accelerate the training of large language models while conserving memory resources. LoRa incorporates pairs of rank-decomposition weight matrices, referred to as update matrices, to existing weights in the model, and only trains these newly added matrices.</p> <ul> <li>Low-Rank Adaptation of Large Language Models (LoRA)</li> <li>Fine-tuning 20B LLMs with RLHF on a 24GB consumer GPU</li> </ul>"},{"location":"Guides/LLM%20Fine-tuning/#quantization","title":"Quantization","text":"<p>Quantization is a method that reduces the computational and memory demands of model inference by representing weights and activations using low-precision data types, like 8-bit integers (int8), instead of the typical 32-bit floating point (float32). Despite the potential advantages, it's worth noting that aggressive quantization can sometimes degrade model performance</p> <ul> <li>Quantization</li> <li>Quantize Transformers models</li> </ul>"},{"location":"Guides/LLM%20Fine-tuning/#models","title":"Models","text":""},{"location":"Guides/LLM%20Fine-tuning/#open-source","title":"Open Source","text":"<p>Open source models can be run on your own hardware and fine-tuned for specialist tasks.</p> <ul> <li>Llama 2: Second version of the Llama model by Meta. Three variations of the model with different sizes have been released: 7B, 13B and 70B. For comparison, the largest Llama model was 65B. In most benchmarks, Llama 2 outperforms other open source LLMs such as Falcon.</li> <li>Dolly 2.0: Developed by Databricks, Dolly 2.0, known as dolly-v2-12b, is an instruction-following model trained on around 15,000 instruction/response records. It was developed on the Databricks machine learning platform using the pythia-12b model. The models can be found here.</li> <li>Falcon: Developed by the Technology Innovation Institute in Abu Dhabi. Has been fine-tuned on a curated subset of the Common Crawl dataset, which the authors have released as the falcon-refinedweb dataset. \u26a0\ufe0f Note that the Huggingface website marks 6 files in this dataset as unsafe (containing trojans). Versions with 1.3B, 7B, 40B and 180B have been released.</li> </ul>"},{"location":"Guides/LLM%20Fine-tuning/#closed-models","title":"Closed models","text":"<p>Closed models can only be used through an (usually paid) API. Some may offer fine-tuning functionality through their API, but more commonly you'd use these models as-is. Currently, GPT4 offers the best performance at most tasks, which makes it useful for benchmarking.</p> <ul> <li>ChatGPT (GPT3.5 and GPT4): Access API here.</li> <li>Claude 2: Developed by Anthropic. Access API here.</li> </ul>"},{"location":"Guides/LLM%20Fine-tuning/#datasets","title":"Datasets","text":""},{"location":"Guides/LLM%20Fine-tuning/#instruction-tuning","title":"Instruction tuning","text":"<p>Instruction-tuning datasets are used for making language models better at responding to user prompts. </p> <ul> <li>awesome-text/visual-instruction-tuning-dataset: A collection of open-source instruction tuning datasets to train (text and multi-modal) chat-based LLMs.</li> <li>Super-Natural Instructions A dataset of expert-created instructions for a wide variety of tasks, including classification, extraction, infilling, sequence tagging, text rewriting, and text composition. See SUPER-NATURAL INSTRUCTIONS: Generalization via Declarative Instructions on 1600+ NLP Tasks for details.</li> <li>unnatural-instructions-core and unnatural-instructions-full: Includes examples created by prompting a language model with three seed examples of instructions and eliciting a fourth. This set is then expanded by prompting the model to rephrase each instruction, creating a total of approximately 240,000 examples of instructions, inputs, and outputs. See Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor for details.</li> <li>Dolly 2.0 Dataset: Dolly 2.0 is a language model fine-tuned on a human-generated instruction dataset. It was developed to exhibit ChatGPT-like human interactivity and instruction-following capabilities, with the dataset serving as a platform for fine-tuning to enhance the model's adherence to instructions\u200b.</li> <li>FLAN datasets: compiles datasets from Flan 2021,\u00a0P3,\u00a0Super-Natural Instructions, along with dozens more datasets into one place, formats them into a mix of zero-shot, few-shot and chain-of-thought templates, then mixes these in proportions that are found to achieve strong results on held-out evaluation benchmarks, as reported for Flan-T5 and Flan-PaLM in the\u00a0Scaling Flan paper\u00a0and\u00a0Flan Collection paper.</li> </ul>"},{"location":"Guides/LLM%20Fine-tuning/#fine-tuning-tools","title":"Fine-tuning tools","text":""},{"location":"Guides/LLM%20Fine-tuning/#mainstream-tools","title":"'Mainstream' tools:","text":"<ul> <li>Huggingface Supervised Fine-tuning Trainer</li> <li>Huggingface Reward Modeling Trainer</li> </ul>"},{"location":"Guides/LLM%20Fine-tuning/#open-source-efficient-fine-tuning-tools","title":"Open source efficient fine-tuning tools:","text":"<ul> <li>AutoGPTQ: LLM quantization package based on GPTQ algorithm.</li> <li>bitsandbytes: A lightweight wrapper around CUDA custom functions, in particular 8-bit optimizers, matrix multiplication (LLM.int8()), and quantization functions.</li> <li>exllama: A standalone Python/C++/CUDA implementation of Llama for use with 4-bit GPTQ weights, designed to be fast and memory-efficient on modern GPUs.</li> <li>llama.cpp: Port of Facebook's LLaMA model in C/C++.</li> <li>text-generation-webui: A Gradio web UI for Large Language Models. Supports transformers, GPTQ, AWQ, llama.cpp (GGUF), Llama models.</li> </ul>"},{"location":"Guides/LLM%20Fine-tuning/#resources","title":"Resources","text":"<p>https://huggingface.co/docs/transformers/perf_train_gpu_one https://huggingface.co/blog/hf-bitsandbytes-integration https://huggingface.co/blog/4bit-transformers-bitsandbytes https://huggingface.co/blog/gptq-integration</p>"},{"location":"Hardware/Meta%20Research%20SuperCluster/","title":"Meta Research SuperCluster","text":"<p>Introductory blog post here.</p>"},{"location":"Models/Claude%202/","title":"Claude 2","text":"<p>LLM released by Anthropic. An evolution of Claude.</p>"},{"location":"Models/Claude%202/#-introductory-blog-post","title":"- Introductory Blog Post","text":""},{"location":"Models/Claude/","title":"Claude","text":"<p>LLM released by Anthropic</p> <ul> <li>Introductory Post</li> <li>Meet Claude: Anthropic\u2019s Rival to ChatGPT</li> <li>Appears to build on Anthropic's previous work described in Constitutional AI: Harmlessness from AI Feedback</li> </ul>"},{"location":"Models/GPT-3.5/","title":"GPT 3.5","text":"<p>LLM released by OpenAI</p>"},{"location":"Models/GPT-4/","title":"GPT 4","text":"<p>LLM released by OpenAI</p>"},{"location":"Models/Llama%202/","title":"Llama 2","text":"<p>Second version of the Llama model by Meta AI. Three variations of the model with different sizes have been released: 7B, 13B and 70B. For comparison, the largest Llama model was 65B. In most benchmarks, Llama 2 outperforms other open source LLMs such as Falcon. No comparison to closed models such as OpenAI have been made yet. The company has published a paper for this model.</p> <p>Llama 2 is released under what they call the Llama 2 community licence. This is a commercial licence with some limitations. Of note: the outputs of the Llama 2 model cannot be used to train other models, and commercial use is restricted to companies with under 700 million monthly active users.</p> <p>With the release of this model, it also becomes apparent that Meta AI is aware of the open source community that has sprung out around its Llama models and the company is trying to turn it into a commercial/research advantage. This is reflected in the fact that along with the model release, Meta AI has also announced the following:</p> <ul> <li>A responsible use guide.</li> <li>The launch of a research community initiative, open to professors of accredited universities.</li> <li>The Llama Impact Challenge, for which few details have been released.</li> <li>A community forum on Generative AI, in collaboration with Stanford Deliberative Democracy Lab and the Behavioural Insights Team.</li> </ul> <p>The release blog post can be found here.</p>"},{"location":"Models/Llama/","title":"Llama","text":"<p>LLM released by Meta AI</p>"},{"location":"Models/MPT/","title":"MPT","text":"<p>LLM by MosaicML. Release blog post here.</p>"},{"location":"Models/PaLM%202/","title":"PaLM 2","text":"<p>LLM released by Google AI. Details here. Release blog post here</p>"},{"location":"Models/PaLM/","title":"PaLM","text":"<p>LLM released by Google AI</p>"},{"location":"Techniques/Quantization/","title":"Quantization","text":"<p>Some resources</p> <p>https://huggingface.co/blog/hf-bitsandbytes-integration https://huggingface.co/blog/4bit-transformers-bitsandbytes</p>"},{"location":"Tools-Libraries/AutoGPTQ/","title":"AutoGPTQ","text":"<p>An LLM quantization packag based on the GPTQ algorithm.</p> <p>Repository</p>"},{"location":"Tools-Libraries/Bard/","title":"Bard","text":"<p>Launched in March 2023, Bard is a chatbot developed by Google AI. It was initially based on the [[LaMDA]] family LLMs and later incorporated the PaLM LLM.</p>"},{"location":"Tools-Libraries/Vector%20Databases/","title":"Vector Databases","text":"<ul> <li>Pinecone</li> <li>Chroma</li> </ul>"},{"location":"Tools-Libraries/exllama/","title":"Exllama","text":"<p>A more memory-efficient rewrite of the HF transformers implementation of Llama for use with quantized weights.</p> <p>Repository</p>"},{"location":"Tools-Libraries/llama.cpp/","title":"Llama.cpp","text":"<p>A port of Facebook's LLaMA model in C/C++. </p> <p>Repository</p>"},{"location":"Tools-Libraries/text-generation-webui/","title":"Text generation webui","text":"<p>Gradio web UI for running LLMs, seeing a lot of use in LLM open source / enthusiast communities such as the LocalLlama subreddit.</p> <p>Repository</p>"}]}